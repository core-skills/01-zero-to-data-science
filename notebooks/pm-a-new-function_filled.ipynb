{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Functions\n",
    "\n",
    "This exercise will lead you through taking some common data processing steps and wrapping them up into a reusable function.\n",
    "\n",
    "The function should import a file, do a bit of processing and formatting, output the processed data elsewhere and return the path to the processed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path # handy for working with file paths, consistent across systems (windows, mac, unix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check we're working with a similar version of pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a small timeseries air quality dataset from Oxford Street in London.\n",
    "\n",
    "This contains hourly-averaged readings of different gas and particulate \n",
    "species in the near-road environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = Path(\"../data/OxfordStreetAirQuality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = Path(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a csv file\n",
    "df = pd.read_csv(data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # could also use .describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some rows where we we're missing some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As missing values don't tend to play nicely with the machine learning steps we might apply later, let's drop the rows with *any* missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is indexed by time, so let's make sure it has the right data type (in this case `datetime64`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data type for the reading datetime\n",
    "df['ReadingDateTime'] = df['ReadingDateTime'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use this as the index for the DataFrame, rather than just another column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index to the reading time\n",
    "df.set_index('ReadingDateTime', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table isn't in the most useful format for what we might want to do with it, for which we need the variables to be columns.\n",
    "\n",
    "Let's pivot it to obtain a table of `Value`s for each `Species` over `Time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot the table \n",
    "pivoted = pd.pivot_table(df, index=['ReadingDateTime'], columns=['Species'], values='Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the data we've imported is aggregated on an hourly basis.\n",
    "\n",
    "Perhaps we're looking at longer term trends, and this level of detail is unecessary.\n",
    "\n",
    "We can `resample()` this data to a weekly(`W`)-`mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a weekly mean\n",
    "weekly_mean = pivoted.resample('W', label='right').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, we can check what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = weekly_mean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a flag for where one of the variables is beyond a certain threshold (you could do something simliar for e.g. data quality).\n",
    "\n",
    "In this case let's add a column called `'Hazardous'` which contains values which are `True` where the column `'NOX'` is above 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a data flag\n",
    "weekly_mean['Hazardous'] = weekly_mean['NOX'] > 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished processing our data file, let's save it to our `output_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a processed data folder if there isn't one already\n",
    "if not output_folder.exists():\n",
    "    output_folder.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output this to a 'processed files' folder\n",
    "output_filepath = output_folder / 'WeeklyMeanAQ.csv'\n",
    "weekly_mean.to_csv(output_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a workflow we can use, let's combine the data processing steps above into a reusable function.\n",
    "\n",
    "We've added the rough structure for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def process_csv_file(filepath, output_folder = Path(\"../data/processed/\"), fill_with=np.nan):\n",
    "    \"\"\"\n",
    "    Process a csv file so it's ready for exploratory data analysis.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    filepath\n",
    "        Path to the csv file to import.\n",
    "    output_folder \n",
    "        Path to the folder where you want the processed version to reside.\n",
    "    fill_with \n",
    "        Value to substitute for zero.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    output_filepath\n",
    "        Path to the csv file which is output.\n",
    "        \n",
    "    Notes\n",
    "    --------\n",
    "    This function will convert data types and fill zeros with the specified value.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath) # replace 'data_filepath' with 'filepath' here\n",
    "    \n",
    "    df.dropna(how='any', axis=0, inplace=True) # remove rows with \n",
    "    \n",
    "    df['ReadingDateTime'] = df['ReadingDateTime'].astype('datetime64')\n",
    "    \n",
    "    df.set_index('ReadingDateTime', inplace=True, drop=True)\n",
    "    \n",
    "    pivoted = pd.pivot_table(df, index=['ReadingDateTime'], columns=['Species'], values='Value')\n",
    "    \n",
    "    weekly_mean = pivoted.resample('W').mean()\n",
    "    \n",
    "    weekly_mean['Hazardous'] = weekly_mean['NOX'] > 50\n",
    "    \n",
    "    if not output_folder.exists():\n",
    "        output_folder.mkdir(parents=True)\n",
    "    \n",
    "    output_filepath = output_folder / 'WeeklyMeanAQ.csv'\n",
    "\n",
    "    weekly_mean.to_csv(output_filepath)\n",
    "    # add the return value\n",
    "    return output_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this function, and the libraries imported above into the separate file `processor.py`. \n",
    "\n",
    "Now when we want to use this function, we can import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_pipeline.processor import process_csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check back to see what arguments the function takes, you can use the inline help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(process_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_csv_file(data_filepath, \n",
    "                 output_folder =Path(\"../data/another_processed_data_folder/\"), \n",
    "                 fill_with=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
